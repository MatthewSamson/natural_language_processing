{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the required text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting anna.txt via online\n",
    "target_url = \"https://raw.githubusercontent.com/udacity/deep-learning/master/tensorboard/anna.txt\"\n",
    "data = request.urlopen(target_url)\n",
    "#print(data)\n",
    "#print(data) # prints the object id and not file content\n",
    "\n",
    "# testing the anna.txt loaded file\n",
    "'''\n",
    "for line in data:\n",
    "    print(line)\n",
    "'''\n",
    "\n",
    "# create anna.txt and write data to it\n",
    "f = open(\"annaKarenina.txt\", \"w+\")\n",
    "i = 0\n",
    "for line in data:\n",
    "    i += 1\n",
    "    #print(line)\n",
    "    bytes2Str = line.decode(\"utf-8\")\n",
    "    #print(bytes2Str)\n",
    "    f.write(bytes2Str)\n",
    "#print(i)\n",
    "f.close()\n",
    "\n",
    "#Open the file back and read the contents\n",
    "\n",
    "f = open(\"annaKarenina.txt\", \"r\")\n",
    "text = f.read()\n",
    "#print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the characters with integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "charList = tuple(set(text))\n",
    "#print(charList)\n",
    "\n",
    "# dictionary mapping integers to each character\n",
    "encodeCharWInt = dict(enumerate(charList))\n",
    "#print(encodeCharWInt)\n",
    "\n",
    "# look up table mapping character to integers\n",
    "lookUp = {char: int_ for int_, char in encodeCharWInt.items()}\n",
    "#print(lookUp)\n",
    "\n",
    "# encode the text\n",
    "enArr = np.array([lookUp[char] for char in text])\n",
    "#print(enArr[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEn(numArr, noOfLabels):\n",
    "    \n",
    "    # init the encoded array\n",
    "    oneHot = np.zeros((np.multiply(*numArr.shape), noOfLabels), dtype=np.float32)\n",
    "    \n",
    "    # encoding appropriate elements with 1 after reshaping\n",
    "    oneHot[np.arange(oneHot.shape[0]), numArr.flatten()] = 1.\n",
    "    \n",
    "    # reshape to get original shape back\n",
    "    oneHot = oneHot.reshape((*numArr.shape, noOfLabels))\n",
    "    \n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = oneHotEn(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch splitting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return batches of size batchSize * seqLen\n",
    "def splitBatches(numArr, batchSize, seqLen):\n",
    "    '''\n",
    "       Arguments\n",
    "       ---------\n",
    "       numArr: array you want to make batches from\n",
    "       batchSize: batch size, the number of sequences per batch\n",
    "       seqLen: number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    totalBatchSize = batchSize * seqLen\n",
    "    \n",
    "    # total number of batches we can make, // integer division, round down\n",
    "    totBatches = len(numArr)//totalBatchSize\n",
    "    \n",
    "    # characters truncated upto completed batches\n",
    "    numArr = numArr[:totBatches * totalBatchSize]\n",
    "    \n",
    "    # reshape based on batchSize\n",
    "    numArr = numArr.reshape((batchSize, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, numArr.shape[1], seqLen):\n",
    "        \n",
    "        # the input features\n",
    "        x = numArr[:, n:n+seqLen]\n",
    "        \n",
    "        # shifting the input by 1 gives the targets\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], numArr[:, n+seqLen]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], numArr[:, 0]\n",
    "            \n",
    "        yield x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test batch splitting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and output truncted to 10x10\n",
      "input matrix\n",
      " [[36 72 56  4  2 82 54 57 13 58]\n",
      " [58  2 72 37 64 78 72  2 28 57]\n",
      " [64 10 78 57 47 56 10 18 43 58]\n",
      " [10 66 57  2 37 57 54 82  2 21]\n",
      " [54 14 57 72 82 57 72 56 66 57]\n",
      " [37 57 66 21 28 24 64 28 28 21]\n",
      " [57 31 37  8  8 74 14 57 66 56]\n",
      " [21 10 57  2 72 82 57 24 37 10]]\n",
      "\n",
      "expected output matrix\n",
      " [[72 56  4  2 82 54 57 13 58 58]\n",
      " [ 2 72 37 64 78 72  2 28 57 37]\n",
      " [10 78 57 47 56 10 18 43 58 58]\n",
      " [66 57  2 37 57 54 82  2 21 54]\n",
      " [14 57 72 82 57 72 56 66 57 10]\n",
      " [57 66 21 28 24 64 28 28 21 37]\n",
      " [31 37  8  8 74 14 57 66 56 54]\n",
      " [10 57  2 72 82 57 24 37 10 24]]\n"
     ]
    }
   ],
   "source": [
    "batchTest = splitBatches(enArr, 8, 50)\n",
    "inputMat, expectedOut = next(batchTest)\n",
    "\n",
    "# printing out the first 10 items in a sequence\n",
    "print(\"Input and output truncted to 10x10\")\n",
    "print('input matrix\\n', inputMat[:10, :10])\n",
    "print('\\nexpected output matrix\\n', expectedOut[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the LSTM within a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=512, n_layers=4,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.charList = tokens\n",
    "        self.encodeCharWInt = dict(enumerate(self.charList))\n",
    "        self.lookUp = {ch: ii for ii, ch in self.encodeCharWInt.items()}\n",
    "        \n",
    "        # define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.charList), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.charList))\n",
    "      \n",
    "    # forward pass through the network\n",
    "    # takes input batch, hidden or cell state as input\n",
    "    def forward(self, x, hidden):\n",
    "                \n",
    "        # get the outputs and the new hidden state from the lstm\n",
    "        r_op, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(r_op)\n",
    "        \n",
    "        # Stack up LSTM outputs using view and contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        # put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def initHidden(self, batchSize):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,initialized to zero, for hidden and cell state\n",
    "        weightTensor = next(self.parameters()).data\n",
    "        hiddenTensor = (weightTensor.new(self.n_layers, batchSize, self.n_hidden).zero_(),\n",
    "                      weightTensor.new(self.n_layers, batchSize, self.n_hidden).zero_())\n",
    "        \n",
    "        return hiddenTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batchSize=10, seqLen=50, lr=0.001, clip=5, valFrac=0.1, printEvery=10):\n",
    "    ''' \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharLSTM network\n",
    "        data: text data to train the network\n",
    "        epochs: number of epochs to train\n",
    "        batchSize: number of mini-sequences per mini-batch, aka batch size\n",
    "        seqLen: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        valFrac: Fraction of data to hold out for validation\n",
    "        printEvery: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # splitting training and validation data\n",
    "    valID = int(len(data)*(1-valFrac))\n",
    "    data, valData = data[:valID], data[valID:]\n",
    "    \n",
    "    # counts steps through training\n",
    "    counter = 0\n",
    "    noOfChars = len(net.charList)\n",
    "    \n",
    "    # begin training\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h = net.initHidden(batchSize)\n",
    "        \n",
    "        for x, y in splitBatches(data, batchSize, seqLen):\n",
    "            counter += 1\n",
    "            \n",
    "            # one-hot encode our data and make them Torch tensors\n",
    "            x = oneHotEn(x, noOfChars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y).type(torch.LongTensor)\n",
    "            \n",
    "\n",
    "            # creating new variables for the hidden state, to prevent back-prop through training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batchSize*seqLen))\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % printEvery == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                valHid = net.initHidden(batchSize)\n",
    "                valLosses = []\n",
    "                net.eval() # set to eval mode\n",
    "                \n",
    "                for x, y in splitBatches(valData, batchSize, seqLen):\n",
    "                    # one-hot encode our data and make them Torch tensors\n",
    "                    x = oneHotEn(x, noOfChars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y).type(torch.LongTensor)\n",
    "                    \n",
    "                    # creating new variables for the hidden state, to prevent back-prop through training history\n",
    "                    valHid = tuple([each.data for each in valHid])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    output, valHid = net(inputs, valHid)\n",
    "                    valLoss = criterion(output, targets.view(batchSize*seqLen))\n",
    "                \n",
    "                    valLosses.append(valLoss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(valLosses)))\n",
    "                print(\"**********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (lstm): LSTM(83, 128, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=128, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=128\n",
    "n_layers=4\n",
    "\n",
    "net = CharLSTM(charList, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 3.9285... Val Loss: 3.7007\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2850... Val Loss: 3.1731\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 30... Loss: 3.2170... Val Loss: 3.1384\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 40... Loss: 3.1924... Val Loss: 3.1277\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 50... Loss: 3.1811... Val Loss: 3.1273\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 60... Loss: 3.1563... Val Loss: 3.1231\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 70... Loss: 3.1943... Val Loss: 3.1213\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 80... Loss: 3.1550... Val Loss: 3.1215\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 90... Loss: 3.1350... Val Loss: 3.1201\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 100... Loss: 3.1548... Val Loss: 3.1202\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 110... Loss: 3.1482... Val Loss: 3.1198\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 120... Loss: 3.1330... Val Loss: 3.1191\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 130... Loss: 3.1452... Val Loss: 3.1195\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 140... Loss: 3.1291... Val Loss: 3.1188\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 150... Loss: 3.1334... Val Loss: 3.1188\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 160... Loss: 3.1219... Val Loss: 3.1179\n",
      "**********************************************\n",
      "Epoch: 1/10... Step: 170... Loss: 3.1272... Val Loss: 3.1184\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 180... Loss: 3.0967... Val Loss: 3.1185\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 190... Loss: 3.1412... Val Loss: 3.1183\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 200... Loss: 3.1100... Val Loss: 3.1183\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 210... Loss: 3.1415... Val Loss: 3.1189\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 220... Loss: 3.1388... Val Loss: 3.1181\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 230... Loss: 3.1268... Val Loss: 3.1189\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 240... Loss: 3.1270... Val Loss: 3.1182\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 250... Loss: 3.1412... Val Loss: 3.1183\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 260... Loss: 3.1378... Val Loss: 3.1182\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 270... Loss: 3.0954... Val Loss: 3.1186\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 280... Loss: 3.1183... Val Loss: 3.1190\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 290... Loss: 3.1094... Val Loss: 3.1184\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 300... Loss: 3.1099... Val Loss: 3.1185\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 310... Loss: 3.1086... Val Loss: 3.1191\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 320... Loss: 3.1273... Val Loss: 3.1183\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 330... Loss: 3.1302... Val Loss: 3.1183\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 340... Loss: 3.1208... Val Loss: 3.1178\n",
      "**********************************************\n",
      "Epoch: 2/10... Step: 350... Loss: 3.1148... Val Loss: 3.1184\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 360... Loss: 3.1236... Val Loss: 3.1185\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 370... Loss: 3.1276... Val Loss: 3.1182\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 380... Loss: 3.1415... Val Loss: 3.1183\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 390... Loss: 3.1393... Val Loss: 3.1185\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 400... Loss: 3.1412... Val Loss: 3.1182\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 410... Loss: 3.1295... Val Loss: 3.1186\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 420... Loss: 3.1278... Val Loss: 3.1181\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 430... Loss: 3.1193... Val Loss: 3.1176\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 440... Loss: 3.1077... Val Loss: 3.1148\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 450... Loss: 3.0975... Val Loss: 3.1088\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 460... Loss: 3.1006... Val Loss: 3.0943\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 470... Loss: 3.0770... Val Loss: 3.0653\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 480... Loss: 3.0408... Val Loss: 3.0220\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 490... Loss: 2.9928... Val Loss: 2.9752\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 500... Loss: 2.9862... Val Loss: 2.9284\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 510... Loss: 2.9035... Val Loss: 2.8958\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 520... Loss: 2.9182... Val Loss: 2.8680\n",
      "**********************************************\n",
      "Epoch: 3/10... Step: 530... Loss: 2.8571... Val Loss: 2.8501\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 540... Loss: 2.8826... Val Loss: 2.8310\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 550... Loss: 2.8387... Val Loss: 2.8079\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 560... Loss: 2.8241... Val Loss: 2.7838\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 570... Loss: 2.8068... Val Loss: 2.7570\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 580... Loss: 2.8047... Val Loss: 2.7291\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 590... Loss: 2.7840... Val Loss: 2.6996\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 600... Loss: 2.7537... Val Loss: 2.6693\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 610... Loss: 2.7185... Val Loss: 2.6473\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 620... Loss: 2.6689... Val Loss: 2.6237\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 630... Loss: 2.6688... Val Loss: 2.6090\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 640... Loss: 2.6785... Val Loss: 2.5961\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 650... Loss: 2.6581... Val Loss: 2.5807\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 660... Loss: 2.6252... Val Loss: 2.5719\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 670... Loss: 2.6403... Val Loss: 2.5643\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 680... Loss: 2.6027... Val Loss: 2.5531\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 690... Loss: 2.5944... Val Loss: 2.5456\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 700... Loss: 2.6054... Val Loss: 2.5385\n",
      "**********************************************\n",
      "Epoch: 4/10... Step: 710... Loss: 2.5824... Val Loss: 2.5293\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 720... Loss: 2.5821... Val Loss: 2.5231\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 730... Loss: 2.5717... Val Loss: 2.5164\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 740... Loss: 2.5971... Val Loss: 2.5086\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 750... Loss: 2.5611... Val Loss: 2.5027\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 760... Loss: 2.5798... Val Loss: 2.4956\n",
      "**********************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10... Step: 770... Loss: 2.5607... Val Loss: 2.4899\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 780... Loss: 2.5444... Val Loss: 2.4840\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 790... Loss: 2.5523... Val Loss: 2.4767\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 800... Loss: 2.5411... Val Loss: 2.4732\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 810... Loss: 2.5328... Val Loss: 2.4658\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 820... Loss: 2.5066... Val Loss: 2.4692\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 830... Loss: 2.5150... Val Loss: 2.4615\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 840... Loss: 2.5022... Val Loss: 2.4552\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 850... Loss: 2.4849... Val Loss: 2.4512\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 860... Loss: 2.5283... Val Loss: 2.4489\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 870... Loss: 2.4992... Val Loss: 2.4432\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 880... Loss: 2.4953... Val Loss: 2.4375\n",
      "**********************************************\n",
      "Epoch: 5/10... Step: 890... Loss: 2.5054... Val Loss: 2.4333\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 900... Loss: 2.4977... Val Loss: 2.4329\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 910... Loss: 2.4917... Val Loss: 2.4315\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 920... Loss: 2.4979... Val Loss: 2.4239\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 930... Loss: 2.4767... Val Loss: 2.4204\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 940... Loss: 2.4747... Val Loss: 2.4182\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 950... Loss: 2.4719... Val Loss: 2.4161\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 960... Loss: 2.5084... Val Loss: 2.4112\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 970... Loss: 2.4593... Val Loss: 2.4081\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 980... Loss: 2.4493... Val Loss: 2.4036\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 990... Loss: 2.4777... Val Loss: 2.4021\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 1000... Loss: 2.4861... Val Loss: 2.3988\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 1010... Loss: 2.4634... Val Loss: 2.3944\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 1020... Loss: 2.4675... Val Loss: 2.3904\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 1030... Loss: 2.4538... Val Loss: 2.3893\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 1040... Loss: 2.4658... Val Loss: 2.3865\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 1050... Loss: 2.4585... Val Loss: 2.3807\n",
      "**********************************************\n",
      "Epoch: 6/10... Step: 1060... Loss: 2.4624... Val Loss: 2.3755\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1070... Loss: 2.4069... Val Loss: 2.3719\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1080... Loss: 2.4632... Val Loss: 2.3690\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1090... Loss: 2.4288... Val Loss: 2.3679\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1100... Loss: 2.4588... Val Loss: 2.3640\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1110... Loss: 2.4432... Val Loss: 2.3585\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1120... Loss: 2.4294... Val Loss: 2.3577\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1130... Loss: 2.3997... Val Loss: 2.3513\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1140... Loss: 2.4327... Val Loss: 2.3480\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1150... Loss: 2.4365... Val Loss: 2.3450\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1160... Loss: 2.3803... Val Loss: 2.3406\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1170... Loss: 2.3927... Val Loss: 2.3430\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1180... Loss: 2.4207... Val Loss: 2.3362\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1190... Loss: 2.4045... Val Loss: 2.3331\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1200... Loss: 2.3999... Val Loss: 2.3314\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1210... Loss: 2.4012... Val Loss: 2.3284\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1220... Loss: 2.3827... Val Loss: 2.3242\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1230... Loss: 2.4008... Val Loss: 2.3205\n",
      "**********************************************\n",
      "Epoch: 7/10... Step: 1240... Loss: 2.4158... Val Loss: 2.3195\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1250... Loss: 2.3910... Val Loss: 2.3135\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1260... Loss: 2.3710... Val Loss: 2.3086\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1270... Loss: 2.4076... Val Loss: 2.3074\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1280... Loss: 2.3976... Val Loss: 2.3021\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1290... Loss: 2.4003... Val Loss: 2.3038\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1300... Loss: 2.4050... Val Loss: 2.3001\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1310... Loss: 2.3659... Val Loss: 2.2977\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1320... Loss: 2.3686... Val Loss: 2.2927\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1330... Loss: 2.3812... Val Loss: 2.2909\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1340... Loss: 2.3537... Val Loss: 2.2886\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1350... Loss: 2.3499... Val Loss: 2.2843\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1360... Loss: 2.3340... Val Loss: 2.2800\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1370... Loss: 2.3503... Val Loss: 2.2764\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1380... Loss: 2.3437... Val Loss: 2.2743\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1390... Loss: 2.3581... Val Loss: 2.2693\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1400... Loss: 2.3513... Val Loss: 2.2672\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1410... Loss: 2.3604... Val Loss: 2.2649\n",
      "**********************************************\n",
      "Epoch: 8/10... Step: 1420... Loss: 2.3304... Val Loss: 2.2603\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1430... Loss: 2.3622... Val Loss: 2.2612\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1440... Loss: 2.3341... Val Loss: 2.2584\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1450... Loss: 2.3205... Val Loss: 2.2593\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1460... Loss: 2.3295... Val Loss: 2.2484\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1470... Loss: 2.3388... Val Loss: 2.2470\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1480... Loss: 2.3485... Val Loss: 2.2437\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1490... Loss: 2.3320... Val Loss: 2.2409\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1500... Loss: 2.3223... Val Loss: 2.2403\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1510... Loss: 2.2870... Val Loss: 2.2347\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1520... Loss: 2.3127... Val Loss: 2.2326\n",
      "**********************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10... Step: 1530... Loss: 2.3317... Val Loss: 2.2337\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1540... Loss: 2.3381... Val Loss: 2.2259\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1550... Loss: 2.2865... Val Loss: 2.2253\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1560... Loss: 2.3165... Val Loss: 2.2231\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1570... Loss: 2.2872... Val Loss: 2.2202\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1580... Loss: 2.2869... Val Loss: 2.2156\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1590... Loss: 2.3045... Val Loss: 2.2149\n",
      "**********************************************\n",
      "Epoch: 9/10... Step: 1600... Loss: 2.2935... Val Loss: 2.2153\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1610... Loss: 2.2901... Val Loss: 2.2103\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1620... Loss: 2.2761... Val Loss: 2.2087\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1630... Loss: 2.3130... Val Loss: 2.2049\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1640... Loss: 2.2733... Val Loss: 2.2025\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1650... Loss: 2.3070... Val Loss: 2.1977\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1660... Loss: 2.2854... Val Loss: 2.1955\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1670... Loss: 2.2645... Val Loss: 2.1964\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1680... Loss: 2.2880... Val Loss: 2.1912\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1690... Loss: 2.2825... Val Loss: 2.1951\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1700... Loss: 2.2721... Val Loss: 2.1884\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1710... Loss: 2.2743... Val Loss: 2.1857\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1720... Loss: 2.2674... Val Loss: 2.1805\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1730... Loss: 2.2575... Val Loss: 2.1807\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1740... Loss: 2.2285... Val Loss: 2.1776\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1750... Loss: 2.2821... Val Loss: 2.1752\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1760... Loss: 2.2600... Val Loss: 2.1744\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1770... Loss: 2.2639... Val Loss: 2.1695\n",
      "**********************************************\n",
      "Epoch: 10/10... Step: 1780... Loss: 2.2739... Val Loss: 2.1657\n",
      "**********************************************\n"
     ]
    }
   ],
   "source": [
    "batchSize = 64\n",
    "seqLen = 160 #max length verses\n",
    "n_epochs = 10 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, enArr, epochs=n_epochs, batchSize=batchSize, seqLen=seqLen, lr=0.001, printEvery=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2a45ea5d272c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodelName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'rnnLong.net'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m checkpoint = {'n_hidden': net.n_hidden,\n\u001b[0m\u001b[0;32m      4\u001b[0m               \u001b[1;34m'n_layers'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               \u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# does not save the model until training is complete\n",
    "# be careful not to save models with same name\n",
    "# set model name here and keep track of it\n",
    "modelName = 'rnnLong.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.charList}\n",
    "\n",
    "print(net.state_dict())\n",
    "with open(modelName, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to predict characters based on the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next character given a particular character\n",
    "def predict(net, char, hidden=None, topKRes=None):\n",
    "        \n",
    "        # tensor inputs\n",
    "        inputTensor = np.array([[net.lookUp[char]]])\n",
    "        inputTensor = oneHotEn(inputTensor, len(net.charList))\n",
    "        inputs = torch.from_numpy(inputTensor)\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # get the output of the model\n",
    "        output, hidden = net(inputs, hidden)\n",
    "\n",
    "        # get the character probabilities using softmax to get p probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        #print(charProb)\n",
    "        \n",
    "        # get top characters considering the k most probable characters\n",
    "        if topKRes is None:\n",
    "            topChar = np.arange(len(net.charList))\n",
    "        else:\n",
    "            p, topChar = p.topk(topKRes)\n",
    "            topChar = topChar.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(topChar, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.encodeCharWInt[char], hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples using the predict function\n",
    "def sampleOp(net, size, prime='that', topKRes=None):\n",
    "    \n",
    "    print(\"prime = \", prime)\n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    # pass prime chars through the predict function\n",
    "    hidden = net.initHidden(1)\n",
    "    for ch in prime:\n",
    "        predChar, hidden = predict(net, ch, hidden, topKRes=topKRes)\n",
    "\n",
    "    chars.append(predChar)\n",
    "    \n",
    "    # next pass in the previous character and get a new one\n",
    "    for val in range(size):\n",
    "        predChar, hidden = predict(net, chars[-1], hidden, topKRes=topKRes)\n",
    "        chars.append(predChar)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in working a the wore thile the seme to she woul she canded. That at the\n",
      "\n",
      "mele hor had so hin wenl an all whone a the sand ont her, a wose to ald so sore her. And the musher thing the castings ant that the sile the\n",
      "\n",
      "will thinger. He cell whas she the sould whonh him his sise one the was a tond the pronce, and the wind her sam out of whas\n",
      "\n",
      "and hen sand, bit of his hind.\n",
      "\n",
      "\n",
      "\n",
      "\"And allyation tore and was worling to her he wam, and., I'd some to white with ander the she to\n",
      "\n",
      "his sterended the was shing of his antangant ans\n",
      "\n",
      "him the prace ware. The cones an him wish she tale.\n",
      "\n",
      "\n",
      "\n",
      "\"What ha ment to his with a sould on the could to shile\n",
      "\n",
      "stonged her had hing alowe of his the wert was have then to saint his some\n",
      "\n",
      "thine her the said hear that hid hinding, wame,, and wouth though her\n",
      "\n",
      "think the saminc of his weal wering how the said, and was was hord and he was his and sound than tome home there sinct tarl him and anding the\n",
      "\n",
      "them. I' wastision,\n",
      "\n",
      "and, and the was hand hould shere siming that whing hin,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# invoking the prediction function\n",
    "generated = sampleOp(net, 1000, prime='in ', topKRes=5)\n",
    "\n",
    "# create generatedText.txt and write data to it\n",
    "f = open(\"lmGeneratedText.txt\", \"w+\")\n",
    "for line in generated:\n",
    "    f.write(line)\n",
    "f.close()\n",
    "\n",
    "# open the file back and read the contents\n",
    "f = open(\"lmGeneratedText.txt\", \"r\")\n",
    "genText = f.read()\n",
    "# print (genText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the name of the model before loading\n",
    "with open('rnn_2l_256n.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharLSTM(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation from a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prime =  in \n",
      "in time, say, and she say hers of shoughed of that here anl she was to and to sale whit him\n",
      "\n",
      "hor at here wist the sis as this.\n",
      "\n",
      "\n",
      "\n",
      "Stay at at wit they walked and ald sall ware him and with his to had and were out at sail, whith a dourd and\n",
      "\n",
      "tingtare.\". The sout her said\n",
      "\n",
      "hive,\n",
      "\n",
      "seenter had and he want of sere the with the more of ho hin sith that that the sispirias of her.\"\n",
      "\n",
      "\n",
      "\n",
      "\"Whing,.\"\n",
      "\n",
      "\n",
      "\n",
      "Ald wat at. Bettold ta him sound the canster on assad of thas the shise and\n",
      "\n",
      "antey hore he wend somented to wely ald the\n",
      "\n",
      "her wat a tarking, and she contertersed his wat and hav alo the said of her thas here at he and seade the porstel to hin he meassers tat he welt to her she selpelt. He\n",
      "\n",
      "and hor her and had har to to so tilk. \"When we he was a say ther to her would the mart and as and the paster then\n",
      "\n",
      "the c\n"
     ]
    }
   ],
   "source": [
    "# sample using a loaded model\n",
    "generated = sampleOp(loaded, 800, topKRes=5, prime=\"in \")\n",
    "\n",
    "# create generatedText.txt and write data to it\n",
    "f = open(\"lmGeneratedText.txt\", \"w+\")\n",
    "for line in generated:\n",
    "    f.write(line)\n",
    "f.close()\n",
    "\n",
    "#Open the file back and read the contents\n",
    "f = open(\"lmGeneratedText.txt\", \"r\")\n",
    "genText = f.read()\n",
    "#print (genText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text structure Accuracy: 48.421052631578945%\n"
     ]
    }
   ],
   "source": [
    "mainFile = []\n",
    "with open('annaKarenina.txt','r') as f:\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "           mainFile.append(word)\n",
    "        \n",
    "uniqueMain = list(set(mainFile))\n",
    "#print(uniqueMain)\n",
    "\n",
    "generatedFile = []\n",
    "with open('lmGeneratedText.txt','r') as f:\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "           generatedFile.append(word)\n",
    "\n",
    "uniqueGenerate = list(set(generatedFile))\n",
    "#print(len(uniqueGenerate))\n",
    "#print(len(mainFile))\n",
    "#print(len(uniqueMain))\n",
    "\n",
    "i = 0\n",
    "for genWord in range(len(uniqueGenerate)):\n",
    "    for word in range(len(uniqueMain)):\n",
    "        if uniqueGenerate[genWord] == uniqueMain[word]:\n",
    "            i += 1\n",
    "            #print(uniqueGenerate[genWord],uniqueMain[word])\n",
    "#print(i)\n",
    "percentAcc = (i/len(uniqueGenerate))*100\n",
    "print(\"Text Structure Accuracy: {}%\".format(percentAcc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
